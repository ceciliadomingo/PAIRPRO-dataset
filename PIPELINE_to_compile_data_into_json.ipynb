{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0ZsW-I2kn7YR",
        "q7aokiwSGrAP",
        "hnNks7hv3ncA",
        "Cu73aTCrCqLP",
        "FjSLydRGCy_X",
        "w9_wXDf1C4aX",
        "M_rNeAtjC95W",
        "r3RciRBXD-ou",
        "DIv74_Ao49nS",
        "shs5MqG4NmGo",
        "imrUa27VH8Rr",
        "dEI9hgK_hOgp",
        "pmW1EkB-hXXf",
        "xxQu3oL1zRtC",
        "_qm7ytVWJeX_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Procedure for processing pilot study data\n",
        "This notebook describes how the data gathered from the pilot study was processed to prepare it for annotation."
      ],
      "metadata": {
        "id": "qcwQCefXFAgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Obtaining the transcripts\n",
        "We tested the quality of several models: Microsoft Teams automatic transcription, Voosk (large model integrated into SubtitleEdit), Azure, and Whisper. We selected Whisper due to its low word error rate (\\~6% on an initial test), especially considering its accuracy even for words that the model can't have seen in training (e.g., unique variable names). The transcripts were manually revised to ensure that we release a dataset of the highest quality. A few errors were corrected, including a few instances of hallucinations (\\~4 lines with repeated phrases). We also adjusted the timestamps, as Whisper's main drawback is that the segmentation is not very precise: many segments included not only the speech, but the surrounding seconds of silence). We used SubtitleEdit for the adjustments."
      ],
      "metadata": {
        "id": "xal1x6YnFaxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Code to obtain the transcripts"
      ],
      "metadata": {
        "id": "0ZsW-I2kn7YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Whisper\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "id": "QKQyVlV4n6B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two ways of obtaining the output:"
      ],
      "metadata": {
        "id": "5Eqys-M7oclR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Through Python code, exporting to a file\n",
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(\"dz.wav\")\n",
        "print(result[\"text\"], file=open(\"OUTPUT FILE.txt\", \"w\"))"
      ],
      "metadata": {
        "id": "zaSPRAQfoSji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Through the terminal. This generates output in json, tsv and srt format.\n",
        "# We used this method to obtain srt files, which are easily modified in SubtitleEdit.\n",
        "!whisper 'AUDIO FILE.wav' --language en --model large"
      ],
      "metadata": {
        "id": "lfVLLhQroj3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing the transcripts\n",
        "For easier processing, we removed two types of lines from the srt files generated by Whisper, after revising the transcripts: subtitle numbers and blank lines. We used Notepad++ for this:\n",
        "<ul>\n",
        "<li>Line operations option to remove blank lines</li>\n",
        "<li>Regex to find and remove subtitle numbers: ^\\d{1,3}\\$ --> replace with blank</li>\n",
        "</ul>\n",
        "<p> We also double checked that all subtitles were in single lines, so that the final final alternated lines with timestamps and lines with subtitles. We used the reged \"[a-z.,]$\\r\\n[a-z\\.,]\" to find newline characters that we had missed in SubtitleEdit, and removed them manually.</p>"
      ],
      "metadata": {
        "id": "hT2PvOI8Go5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Obtaining the speaker labels\n",
        "The transcription model, while very good at speech to text, did not include speaker recognition. Therefore, we needed to use a separate diarization model to automate the assignment of speaker labels. We tried the [pyannote diarization model](https://huggingface.co/pyannote/speaker-diarization) and were satisfied with the results on an initial test. The model only struggled with non-word sounds (e.g., laughter, sighs, coughing, etc.), which were labeled as a new speaker. There was also one instance of the model mistaking the researcher for a participant (same gender, similar age). One issue that we encountered is that the diarization model segments the dialogue differently from the transcription model. For instance, when there is overlap, the diarization model will generate more segments, trying to separate the spakers; the transcription model, on the other hand, may ignore ignore the less audible speaker, or merge both speakers' speech into one utterance if the second speaker did not say enough distinguishable words to warrant a separate segment.<br/>\n",
        "Therefore, we may consider alternative ways of assigning speaker labels:\n",
        "<ul>\n",
        "<li>Manual labelling: too time-consuming.</li>\n",
        "<li>Alternative ASR that includes diarization: while this might improve the speaker label accuracy, it could increase the word error rate.</li>\n",
        "<li>Recording speakers separately: this would require both speakers using microphones, as well as a tool to record the meeting audio from the microphones instead of the output channel; this tool would need to output an absolute timestamp with the recording, in order to synchronize both speakers' files. In a truly remote setting, we would also need to ask speakers about their computer clocktime before the meeting, lest they turn out to be in different timezones.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "S7SaTRMmIi8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the code used to obtain the diarization files, taken from the huggingface website. The output is an rttm file that shows the timestamps in seconds, following the format: <br/>\n",
        "SPEAKER FILE 1 0.498 10.041 <NA> <NA> SPEAKER_00 <NA> <NA>"
      ],
      "metadata": {
        "id": "Vr9X1e3spIJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#From: https://huggingface.co/pyannote/speaker-diarization\n",
        "from pyannote.audio import Pipeline\n",
        "\n",
        "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\",\n",
        "                                    use_auth_token=\"INSERT YOUR HUGGINGFACE TOKEN\")\n",
        "# Make sure to accept Huggingface's terms and conditions for the token to be valid (tokens may be generated before they are valdated)\n",
        "\n",
        "# Apply the pipeline to an audio file\n",
        "diarization = pipeline('SESSION AUDIO.wav')\n",
        "\n",
        "# Dump the diarization output to disk using RTTM format\n",
        "with open(\"SESSION AUDIO.rttm\", \"w\") as rttm:\n",
        "    diarization.write_rttm(rttm)"
      ],
      "metadata": {
        "id": "pvM4NT3pFW7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing the rttm files\n",
        "We used Notepad++ to remove unnecessary columns from the file (filename, NA labels). After comparing the output with the audio file, we also changed the automatic speaker labels into our own speaker code numbers. Each file thus has two speaker code numbers, one RESEARCHER label, and one OTHER label (for non-word segments, which are normally not assigned to any particular speaker)."
      ],
      "metadata": {
        "id": "YMGQUabsjdAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating the json files\n",
        "We combined our data sources into single json files for easier processing. The file sturcutre is as follows:\n",
        "<ul>\n",
        "<li>SESSION object</li>\n",
        "  <ul>\n",
        "  <li>Session ID label</li>\n",
        "  <li>TURNS object, containing a list of TURN objects. Each TURN is a dicitonary with several possible objects:</li>\n",
        "    <ul>\n",
        "    <li>TRANSCRIPT</li>\n",
        "    <li>TIME object, with a dictionary containing a START and END key</li>\n",
        "    <li>DIARIZATION object, with a dictionary containing a RTTM_START, RTTM_END and SPEAKER label</li>\n",
        "    <li>KEYLOGS, containing a list of KEYLOG strings</li>\n",
        "    <li>CODE ENTRIES object, containing a list of dictionaries; each of those is a code entry with the keys: TIME, FILE, CODE and TREE</li>\n",
        "    </ul>\n",
        "  </ul>\n",
        "</ul>\n"
      ],
      "metadata": {
        "id": "q7aokiwSGrAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create the initial structure of the json file from the srt transcription file created by the Whisper model, which we then edited. We used the function below:"
      ],
      "metadata": {
        "id": "8FAkALRCtyPB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGsIZSXxFADY"
      },
      "outputs": [],
      "source": [
        "#Function to take srt file and create json with:\n",
        "#session id\n",
        "    #turn objects with turn ID\n",
        "        #transcript objects within the turns\n",
        "        #time objects within the turns\n",
        "            #start and end times within the time objects\n",
        "\n",
        "def create_annotation_json(srtfilename, srt_filepath):\n",
        "    import json #to create json files\n",
        "    import re #to use regular expression to find information in strings\n",
        "\n",
        "    #Extract session id from srt filename using regex\n",
        "    session_id = re.search('(0\\d\\d[_&]0\\d\\d)', srtfilename).group(1)\n",
        "\n",
        "    #Create json object\n",
        "    annotation_json = {\n",
        "        \"SESSION\": {\n",
        "            \"sessionID\": session_id,\n",
        "            \"TURNS\": []\n",
        "            }\n",
        "    }\n",
        "\n",
        "    #Iteate through srt file\n",
        "    with open(srt_filepath, \"r\") as file:\n",
        "        #Select turns object as location to add each turn\n",
        "        turns = annotation_json['SESSION']['TURNS']\n",
        "\n",
        "        #Read srt lines\n",
        "        lines = file.readlines()\n",
        "        #Read lines two by two, as first of pair is timestamps, and second of pair is subtitle string\n",
        "        for line1, line2 in zip(lines[::2], lines[1::2]):\n",
        "\n",
        "            #Extract transcript\n",
        "            transcript = line2\n",
        "\n",
        "            #Extract timestamps; the line looks like this: 00:01:02,000 --> 00:01:05,000\n",
        "            start_time = re.search('((\\d\\d:\\d\\d:\\d\\d,\\d\\d\\d) --> (\\d\\d:\\d\\d:\\d\\d,\\d\\d\\d))', line1).group(2)\n",
        "            end_time = re.search('((\\d\\d:\\d\\d:\\d\\d,\\d\\d\\d) --> (\\d\\d:\\d\\d:\\d\\d,\\d\\d\\d))', line1).group(3)\n",
        "\n",
        "            #Add TURN object to file\n",
        "            turns.append({\n",
        "            \"TURN\": {\n",
        "                \"TRANSCRIPT\": transcript,\n",
        "                \"TIME\": {\"START\": start_time, \"END\": end_time}\n",
        "                }\n",
        "            })\n",
        "\n",
        "        #Create output filename\n",
        "        json_filename = srtfilename.strip(\"srt\") + \"json\"\n",
        "        #Print output to a json file\n",
        "        print(json.dumps(annotation_json, indent=4), file=open(json_filename, \"+a\"))\n",
        "        print('Generated json file: ' + json_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial structure looks like this example of two turns:<br/>![Initial json.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "q0ZNMCO9vg3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adding speaker labels\n",
        "We used the [pyannote diarization model](https://huggingface.co/pyannote/speaker-diarization) to automatically obtain speaker labels. The model outputs results in an rttm file.<br/>\n",
        "We edited the files on Notepad++ as described above. We then used them to add speaker labels to the json files with the scripts below, which:\n",
        "- Convert the timestamps from seconds into minutes and seconds.\n",
        "- Convert them into the same format as the Whisper srt timestamps.\n",
        "- Convert timestamps to datetime objects in order to compare them (i.e., see which time comes before which, instead of treating them as strings).\n",
        "- Extract the timestampts from the rttm file to perform these transfromations.\n",
        "- Compare rttm and srt timestamps to see which turn from the rttm file was closest to each srt turn. The transcription and diarization models did not segment the dialogues the exact same way, so we needed to compare the turns to find the closest and attach the speaker label from that one.\n",
        "- Add the speaker label to each turn of the json file. We also add the timestamps from the diarization for easier inspection."
      ],
      "metadata": {
        "id": "hnNks7hv3ncA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert to minutes and seconds"
      ],
      "metadata": {
        "id": "Cu73aTCrCqLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCTION TO CONVERT TIMESTAMPS TO MINUTES AND SECONDS\n",
        "\n",
        "#Example of initial format:\n",
        "#SPEAKER 003&009_audio_bleeped 1 1.898 2.312 <NA> <NA> SPEAKER_003 <NA> <NA>\n",
        "\n",
        "#Example of final format:\n",
        "#Start: 00:01, End: 00:04, Speaker: 003\n",
        "\n",
        "#Mini-function to convert time to minutes and seconds\n",
        "def convert_time(seconds):\n",
        "    minutes = int(seconds // 60)\n",
        "    seconds = int(seconds % 60)\n",
        "    return f\"{minutes:02d}:{seconds:02d}\"\n",
        "\n",
        "#----------\n",
        "# MAIN\n",
        "#----------\n",
        "def convert_time_to_mins_in_file(rttm_filepath):\n",
        "  with open(filename, \"r\") as file:\n",
        "    for line in file:\n",
        "      parts = line.strip().split() #Dividing the line of the rttm file into its components. Timestamp is parts 3 and 4, speaker label is part 7\n",
        "      start_time = float(parts[3])\n",
        "      end_time = start_time + float(parts[4])\n",
        "      speaker_code = parts[7]\n",
        "\n",
        "      #Apply time conversion function\n",
        "      formatted_start = convert_time(start_time)\n",
        "      formatted_end = convert_time(end_time)\n",
        "\n",
        "      #Print output to file\n",
        "      output_filename = rttm_filepath[:-5] + \"_formatted.rttm\"\n",
        "      print(f\"Start: {formatted_start}, End: {formatted_end}, Speaker: {speaker_code}\", file=open(output_filename, \"a\"))\n"
      ],
      "metadata": {
        "id": "kk4LI0pm8wLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert rttm timestamps to srt format"
      ],
      "metadata": {
        "id": "FjSLydRGCy_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCTION TO CONVERT RTTM TIMESTAMPS INTO SRT TIMESTAMP FORMAT STRINGS\n",
        "def convert_rttm_timestamps_to_srt_timestamps(rttm_timestamp):\n",
        "  from datetime import datetime, timedelta #For converting timestamp format and operating with timestamps\n",
        "\n",
        "  # Parse the input timestamp to get minutes and seconds\n",
        "  minutes, seconds = map(int, rttm_timestamp.split(\":\"))\n",
        "\n",
        "  # Create a timedelta representing the given minutes and seconds\n",
        "  time_delta = timedelta(minutes=minutes, seconds=seconds)\n",
        "\n",
        "  # Create a reference datetime with zero hours\n",
        "  reference_time = datetime.strptime(\"00:00:00\", \"%H:%M:%S\")\n",
        "\n",
        "  # Add the timedelta to the reference time to get the new time\n",
        "  new_time = reference_time + time_delta\n",
        "\n",
        "  # Format the new time as \"hours:minutes:seconds,milliseconds\"\n",
        "  formatted_new_time = new_time.strftime(\"%H:%M:%S,%f\")[:-3]  # Truncate to milliseconds\n",
        "  return formatted_new_time"
      ],
      "metadata": {
        "id": "sl-GMsYA_lJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert timestamp strings into datetime objects"
      ],
      "metadata": {
        "id": "w9_wXDf1C4aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCTION TO CONVERT TIMESTAMP STRINGS INTO DATETIME OBJECTS\n",
        "def convert_timestamp_string_to_datetime(timestamp_string):\n",
        "  from datetime import datetime, timedelta #For converting timestamp format and operating with timestamps\n",
        "\n",
        "  #Convert timestamp string into datetime object\n",
        "  timestamp_datetime = datetime.strptime(timestamp_string, '%H:%M:%S,%f')\n",
        "  return timestamp_datetime"
      ],
      "metadata": {
        "id": "4Dd-QNYJAN8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extract timestamps from rttm file and apply transformations"
      ],
      "metadata": {
        "id": "M_rNeAtjC95W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCTION TO EXTRACT TIMESTAMPS FROM RTTM FILE\n",
        "#CONVERT START AND END TIMESTAMPS AND EXTRACT SPEAKER\n",
        "def extract_rttm_timestamps(rttmfilename):\n",
        "  import re #For regular expressions to find timestamps in RTTM file\n",
        "  rttm_timestamps_list = [] #List of lists with start and end timestamps and speaker\n",
        "  with open(rttmfilename, 'r') as rttm_file:\n",
        "    for line in rttm_file:\n",
        "      #Regexes to find start and end timestamps and speaker in each line\n",
        "      start_match = re.search('Start: (\\d\\d:\\d\\d)', line)\n",
        "      end_match = re.search('End: (\\d\\d:\\d\\d)', line)\n",
        "      speaker_match = re.search('Speaker: (.*)', line)\n",
        "\n",
        "      #If the regex works, extract results\n",
        "      if start_match and end_match and speaker_match:\n",
        "        start = start_match.group(1)\n",
        "        end = end_match.group(1)\n",
        "        speaker = speaker_match.group(1)\n",
        "\n",
        "        #Select the lines with a recognized speaker (not OTHER) to add them to the list\n",
        "        if speaker != 'OTHER':\n",
        "          #Convert timestamps into srt format (as string)\n",
        "          start_formatted_string = convert_rttm_timestamps_to_srt_timestamps(start)\n",
        "          end_formatted_string = convert_rttm_timestamps_to_srt_timestamps(end)\n",
        "          rttm_timestamps_list.append([start_formatted_string,end_formatted_string,speaker])\n",
        "\n",
        "  return rttm_timestamps_list"
      ],
      "metadata": {
        "id": "z6b8nJUjAp4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Find closest timestamps"
      ],
      "metadata": {
        "id": "r3RciRBXD-ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#----------\n",
        "# MAIN\n",
        "#----------\n",
        "#FUNCTION TO FIND CLOSEST RTTM TIMESTAMP TO SRT TIMESTAMP\n",
        "def find_closest_rttm_timestamp(srt_startORend_time_datetime, rttm_timestamps_list,startORendLABEL): #(Arguments are added automatically on next function)\n",
        "  from datetime import datetime, timedelta #For converting timestamp format and operating with timestamps\n",
        "\n",
        "  #Deciding which timestamp to extract from each pair in the rttm list\n",
        "  #rttm_timestamps_list is a list of lists; each list contains start and end timestamps [[start,end]]\n",
        "  if startORendLABEL == 'start':\n",
        "    index = 0\n",
        "  elif startORendLABEL == 'end':\n",
        "    index = 1\n",
        "\n",
        "  # Initialize variables to keep track of the two closest datetimes and their time differences\n",
        "  closest_datetimes = [None, None]\n",
        "  min_time_differences = [timedelta.max, timedelta.max]\n",
        "\n",
        "  #Iterate through all rttm timestamps and find the one closest to the srt timestamp under consideration\n",
        "  for rttm_timestamp in rttm_timestamps_list:\n",
        "    #Compare rttm timestamp with srt timestamp\n",
        "    rttm_datetime = datetime.strptime(rttm_timestamp[index], '%H:%M:%S,%f') #Convert rttm timestampt to datetime object to operate with it\n",
        "    time_difference = abs(srt_startORend_time_datetime - rttm_datetime)\n",
        "\n",
        "    #If the difference is smaller than the smallest difference so far, update the closest datetimes\n",
        "    #If it's smaller than the smallest difference so far\n",
        "    if time_difference < min_time_differences[0]:\n",
        "      #Now second smallest difference has values of previous smallest difference\n",
        "      min_time_differences[1] = min_time_differences[0]\n",
        "      #And second best rttm timestamp has value of previous best rttm timestamp\n",
        "      closest_datetimes[1] = closest_datetimes[0]\n",
        "\n",
        "      #And now the smallest difference has the value of the new smallest difference\n",
        "      min_time_differences[0] = time_difference\n",
        "      #And the best rttm timestamp has the value of the new best rttm timestamp\n",
        "      closest_datetimes[0] = rttm_timestamp\n",
        "\n",
        "    #If it's not smallest, but second smallest\n",
        "    elif time_difference < min_time_differences[1]:\n",
        "      #Now second smallest difference has value of new second smallest difference\n",
        "      min_time_differences[1] = time_difference\n",
        "      #And second best rttm timestamp has value of new second best rttm timestamp\n",
        "      closest_datetimes[1] = rttm_timestamp\n",
        "\n",
        "  #Return the two closest datetimes\n",
        "  return closest_datetimes"
      ],
      "metadata": {
        "id": "Zg4VmJlHBwwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Apply timestamp conversion and comparison functions to add speaker labels to json"
      ],
      "metadata": {
        "id": "YhgFf7b8H_X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCTION TO ITERATE THROUGH JSON, FIND TIMESTAMPS, AND ADD DIARIZATION\n",
        "def json_and_diarization_combiner(jsonfilename, rttmfilename):\n",
        "  import json #For reading JSON files\n",
        "  from datetime import datetime, timedelta #For converting timestamp format and operating with timestamps\n",
        "  #Read and load the JSON file\n",
        "  with open(jsonfilename, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "  #Iterate through json file\n",
        "  for turn in data['SESSION']['TURNS']:\n",
        "    #Extract timestamps from JSON file (srt format)\n",
        "    srt_start_time = turn['TURN']['TIME']['START']\n",
        "    srt_end_time = turn['TURN']['TIME']['END']\n",
        "    #Convert srt timestamp to datetime object\n",
        "    srt_start_time_datetime = datetime.strptime(srt_start_time, '%H:%M:%S,%f')\n",
        "    srt_end_time_datetime = datetime.strptime(srt_end_time, '%H:%M:%S,%f')\n",
        "\n",
        "    #Iterate through rttm timestamps\n",
        "    rttm_timestamps_list = extract_rttm_timestamps(rttmfilename) #Function that returns list of timestamps\n",
        "\n",
        "    #These functions each return a list with the two rttm lines with timestamps that are closest to our current turn's beginning or end\n",
        "    closest_start_datetimes = find_closest_rttm_timestamp(srt_start_time_datetime, rttm_timestamps_list,startORendLABEL='start')\n",
        "    closest_end_datetimes = find_closest_rttm_timestamp(srt_end_time_datetime, rttm_timestamps_list,startORendLABEL='end')\n",
        "\n",
        "    #Select best rttm timestamp from up to four candidates\n",
        "    #If there is a candidate that is both the closest to the turn's beginning and the closest to its end, then it's the best candidate\n",
        "    if closest_start_datetimes[0] == closest_end_datetimes[0]:\n",
        "      best_rttm_timestamp = closest_start_datetimes[0]\n",
        "    #Otherwise, we need to calculate the combined time difference for each candidate\n",
        "    else:\n",
        "      #Calculate time difference for each candidate:\n",
        "      #Sum of the difference between the srt start time and the rttm start time, plus the difference between the srt end time and the rttm end time\n",
        "      time_difference_candidate1 = abs(srt_start_time_datetime - convert_timestamp_string_to_datetime(closest_start_datetimes[0][0])) + abs(srt_end_time_datetime - convert_timestamp_string_to_datetime(closest_start_datetimes[0][1]))\n",
        "      time_difference_candidate2 = abs(srt_start_time_datetime - convert_timestamp_string_to_datetime(closest_start_datetimes[1][0])) + abs(srt_end_time_datetime - convert_timestamp_string_to_datetime(closest_start_datetimes[1][1]))\n",
        "      time_difference_candidate3 = abs(srt_start_time_datetime - convert_timestamp_string_to_datetime(closest_end_datetimes[0][0])) + abs(srt_end_time_datetime - convert_timestamp_string_to_datetime(closest_end_datetimes[0][1]))\n",
        "      time_difference_candidate4 = abs(srt_start_time_datetime - convert_timestamp_string_to_datetime(closest_end_datetimes[1][0])) + abs(srt_end_time_datetime - convert_timestamp_string_to_datetime(closest_end_datetimes[1][1]))\n",
        "\n",
        "      #Find the smallest time difference\n",
        "      smallest_time_difference = min(time_difference_candidate1, time_difference_candidate2, time_difference_candidate3, time_difference_candidate4)\n",
        "\n",
        "      #Select as best rttm the one with the smallest time difference with the srt timestamps\n",
        "      if smallest_time_difference == time_difference_candidate1:\n",
        "        best_rttm_timestamp = closest_start_datetimes[0]\n",
        "      elif smallest_time_difference == time_difference_candidate2:\n",
        "        best_rttm_timestamp = closest_start_datetimes[1]\n",
        "      elif smallest_time_difference == time_difference_candidate3:\n",
        "        best_rttm_timestamp = closest_end_datetimes[0]\n",
        "      elif smallest_time_difference == time_difference_candidate4:\n",
        "        best_rttm_timestamp = closest_end_datetimes[1]\n",
        "\n",
        "    #Add diarization to json file\n",
        "    #First create content of object as dictionary. Timestamps are the first two elements of the best_rttm_timestamp object, the speaker label is the third.\n",
        "    diarization_dict = {\n",
        "      'RTTM_START': best_rttm_timestamp[0],\n",
        "      'RTTM_END': best_rttm_timestamp[1],\n",
        "      'SPEAKER': best_rttm_timestamp[2]\n",
        "    }\n",
        "    #Then add diarization object inside the current json turn object\n",
        "    turn['TURN']['DIARIZATION'] = diarization_dict\n",
        "\n",
        "  #After groing through all turns, save the json file\n",
        "  with open(jsonfilename, 'w') as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n",
        "  print('JSON file with diarization saved as: ' + jsonfilename)\n"
      ],
      "metadata": {
        "id": "49JlXrdCIRVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adding the keylogger data\n",
        "We recorded participants' keyboard and mouse activity using the [RUI](https://acs.ist.psu.edu/projects/RUI/) tool from Pennsylvania State University, as well as a custom script for Mac users."
      ],
      "metadata": {
        "id": "sSOVmHuSzfX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Keylogger for Mac"
      ],
      "metadata": {
        "id": "DIv74_Ao49nS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SCRIPT TO RECORD KEYBOARD ACTIVITY\n",
        "from pynput import keyboard #to record the keyboard\n",
        "import datetime #to record timestamps\n",
        "\n",
        "#SET DEFAULT VALUE FOR RECORDING\n",
        "recording = True\n",
        "\n",
        "#SETTINGS FOR KEYLOGGER\n",
        "def on_press(key):\n",
        "    #Start listening\n",
        "    with open(\"OUTPUT FILE.txt\", \"a\") as f:\n",
        "        f.write(str(key) + '    ' + str(datetime.datetime.now()) + '\\n')\n",
        "\n",
        "def on_release(key):\n",
        "    # Stop listening\n",
        "    if recording == False:\n",
        "        return False\n",
        "\n",
        "#----------\n",
        "# MAIN\n",
        "#----------\n",
        "#RECORD\n",
        "def start_recording():\n",
        "    if recording == True:\n",
        "        with keyboard.Listener(on_press=on_press, on_release=on_release) as listener:\n",
        "            listener.join()\n",
        "\n",
        "#Call function to start recording\n",
        "start_recording()"
      ],
      "metadata": {
        "id": "suUGiEgA5ZZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SCRIPT TO RECORD MOUSE ACTIVITY\n",
        "from pynput import mouse #to record the mouse\n",
        "import datetime #to record timestamps\n",
        "\n",
        "#OUTPUT EXAMPLE\n",
        "#Mouse: Move(x=1105, y=634) Time: 2023-04-20 16:33:43.866504\n",
        "#Mouse: Click(x=492, y=278, button=Button.left, pressed=True) Time: 2023-04-20 16:33:44.730328\n",
        "\n",
        "\n",
        "#DEFINING OUTPUT FORMAT\n",
        "#If the user moves the mouse\n",
        "def on_move(x, y):\n",
        "    with open(\"OUTPUT FILE.txt\", \"a\") as f:\n",
        "        f.write(\"Mouse: Move%s Time: %s \\n\" %((x, y),datetime.datetime.now()))\n",
        "\n",
        "#If the user clicks\n",
        "def on_click(x, y, button, pressed):\n",
        "    with open(\"OUTPUT FILE.txt\", \"a\") as f:\n",
        "        if pressed:\n",
        "            f.write(\"Mouse: Click%s Time: %s \\n\" %((x, y,button,pressed),datetime.datetime.now()))\n",
        "        else:\n",
        "            f.write(\"Mouse: Released%s Time: %s \\n\" %((x,y,button,pressed),datetime.datetime.now()))\n",
        "\n",
        "#----------\n",
        "# MAIN\n",
        "#----------\n",
        "#RECORD\n",
        "def start_recording():\n",
        "    with mouse.Listener(\n",
        "            on_move=on_move,\n",
        "            on_click=on_click) as listener:\n",
        "        listener.join()\n",
        "\n",
        "#Call function to start recording\n",
        "start_recording()"
      ],
      "metadata": {
        "id": "WIpzMpB3Hify"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing Mac keylogger output\n",
        "The libraries used in our Mac keylogger scripts generate output in a specific way, so we needed to use other scripts to transform the output format to the same as that of the RUI keylogger. The first step was merging the output files - using mroe than one listener in the same script caused Mac computers to crash, so we had to run the keylogger as separate scripts simultaneously."
      ],
      "metadata": {
        "id": "shs5MqG4NmGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCTION TO MERGE OUTPUT FROM MAC KEYLOGGER SCRIPTS\n",
        "import re #to find data in txt string\n",
        "import csv #to output csv\n",
        "from datetime import datetime #to process timestamps\n",
        "\n",
        "# Function to parse mouse log entries\n",
        "def parse_mouse_log(entry):\n",
        "  #Search for elements in txt file line\n",
        "  match = re.search(r'Mouse: (\\w+)\\((.*?)\\) Time: (.*?)$', entry)\n",
        "  if match:\n",
        "    mouse_action = match.group(1) #return mouse action (move, click)\n",
        "    mouse_params = match.group(2) #return additional info for mouse action (left, right)\n",
        "    timestamp = match.group(3) #return timestamp\n",
        "    return mouse_action, mouse_params, timestamp\n",
        "  #If nothing found, return nothing\n",
        "  return None, None, None\n",
        "\n",
        "# Function to parse keyboard log entries\n",
        "def parse_keyboard_log(entry):\n",
        "  parts = entry.split()\n",
        "  #If an actual entry is found\n",
        "  if len(parts) >= 2:\n",
        "    word_and_key = parts[0] #the first bit is the word \"Key\", followed by the key that was pressed\n",
        "    try: #Extract the key from e.g., \"Key.P\"\n",
        "      key_search = re.search(r'Key\\.(.*)', word_and_key)\n",
        "      key = key_search.group(1)\n",
        "    except:\n",
        "      key = word_and_key.strip(\"'\") #the key may be separated by ' instead\n",
        "    #Extract timestamp\n",
        "    timestamp = parts[1] + ' ' + parts[2]\n",
        "    return key, timestamp\n",
        "  #If nothing is found, return nothing\n",
        "  return None, None\n",
        "\n",
        "#----------\n",
        "# MAIN\n",
        "#----------\n",
        "\n",
        "#Function to read mouse and keyboard log and merge them\n",
        "def merge_mac_keylogs(mouse_log,keyboard_log,csv_filename) #Arguments: mouse keylog file, keyboard keylog file, desired csv output file\n",
        "  # Read and process mouse log\n",
        "  mouse_data = []\n",
        "  with open(mouse_log, 'r') as mouse_file:\n",
        "    for line in mouse_file:\n",
        "      # Apply function that extracts data from each line\n",
        "      mouse_action, mouse_params, timestamp = parse_mouse_log(line)\n",
        "      # If data is found, add it to list\n",
        "      if mouse_action and mouse_params and timestamp:\n",
        "        mouse_data.append((mouse_action, mouse_params, timestamp))\n",
        "\n",
        "  # Read and process keyboard log\n",
        "  keyboard_data = []\n",
        "  with open(keyboard_log, 'r') as keyboard_file:\n",
        "    for line in keyboard_file:\n",
        "      # Apply function that extracts data from each line\n",
        "      key, timestamp = parse_keyboard_log(line)\n",
        "      # If data is found, add it to list\n",
        "      if key and timestamp:\n",
        "        keyboard_data.append((key, timestamp))\n",
        "\n",
        "  # Merge mouse and keyboard data, making necessary format changes\n",
        "  merged_data = []\n",
        "\n",
        "  # Process mouse data\n",
        "  for mouse_entry in mouse_data:\n",
        "    mouse_action, mouse_params, mouse_timestamp = mouse_entry\n",
        "    mouse_timestamp = mouse_timestamp.strip() # Remove trailing whitespace from mouse_timestamp, else datetime conversion will fail\n",
        "    # Extract mouse movements and round the coordinates (initial format is Move(791.9664306640625, 459.5268249511719))\n",
        "    if mouse_action == 'Move':\n",
        "      # Find coordinates\n",
        "      x, y = re.search(r'(.*?), (.*)', mouse_params).groups()\n",
        "      # Round them to desired format (no decimals)\n",
        "      rounded_x = round(float(x), 0)\n",
        "      rounded_y = round(float(y), 0)\n",
        "      merged_data.append((mouse_timestamp, 'Move', '', '', '', rounded_x, rounded_y))\n",
        "    # Extract mouse clicks and find which button was pressed (initial format is Click(1107.2633056640625, 242.18179321289062, <Button.left: ((1, 2, 6), 0)>, True))\n",
        "    elif mouse_action == 'Click':\n",
        "      # Find which button was pressed\n",
        "      button = re.search(r'Button\\.(.*?):', mouse_params).group(1)\n",
        "      merged_data.append((mouse_timestamp, 'Pressed', '', '', button.capitalize(), '', '', ''))\n",
        "\n",
        "  # Process keyboard data\n",
        "  for keyboard_entry in keyboard_data:\n",
        "    key, keyboard_timestamp = keyboard_entry\n",
        "    merged_data.append((keyboard_timestamp, 'Key', key.capitalize(), '', '', '', '', ''))\n",
        "\n",
        "  # Write merged data to CSV\n",
        "  csv_columns = ['Time', 'Action', 'Key', 'Modifier', 'Mouse', 'X', 'Y']\n",
        "\n",
        "  with open(csv_filename, 'w', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow(csv_columns)\n",
        "    csv_writer.writerows(merged_data)\n",
        "\n",
        "  print(f'Data has been merged and saved to {csv_filename}')\n"
      ],
      "metadata": {
        "id": "ilfJXixgOBcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCTION TO ORDER THE MERGED MAC KEYLOGGER OUTPUT BY TIMESTAMP\n",
        "def sort_merged_mac_keylog_by_time(csv_file_path, desired_output_file): #Arguments: csv file containing merged Mac keylogger output, filename for desired output file\n",
        "  import pandas as pd #to use csv\n",
        "  from datetime import datetime #to process timestamps\n",
        "  import numpy as np #to sort data\n",
        "\n",
        "  # Read the CSV file\n",
        "  df = pd.read_csv(csv_file_path)\n",
        "\n",
        "  # Convert the timestamp column to datetime format\n",
        "  df['Time'] = pd.to_datetime(df['Time']).dt.floor('ms')\n",
        "\n",
        "  # Sort the DataFrame based on the timestamp column\n",
        "  # Get the indices that would sort the 'Time' column\n",
        "  sort_indices = np.argsort(df['Time'])\n",
        "  # Use the indices to rearrange the rows of the entire DataFrame\n",
        "  df_sorted = df.iloc[sort_indices]\n",
        "\n",
        "  #Save the new df to a csv file\n",
        "  df_sorted.to_csv(desired_output_file, index=False)"
      ],
      "metadata": {
        "id": "m_NP_oGueCbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then use Notepad++ to change the format of the Timestamp column (easier than a Python script).<br/>\n",
        "\n",
        "**Replace milisecond dot in timestamp with colon**<br/>\n",
        "(\\d\\d:\\d\\d:\\d\\d).(\\d\\d\\d,)<br/>\n",
        "\\\\$1:\\\\$2<br/>\n",
        "\n",
        "**Add T to timestamp from Mac keylogger**<br/>\n",
        "2023-05-22 <br/>\n",
        "2023-05-22T<br/>\n",
        "\n",
        "**Make x and y coordinates integers instead of floats ending in .0**<br/>\n",
        "\n",
        "(Move,,,,\\d\\d\\d).0(,\\d\\d\\d).0<br/>\n",
        "\\\\$1\\\\$2"
      ],
      "metadata": {
        "id": "lo0NOMK8fv6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing keylogger output\n",
        "Even though all participants with a Windows computer used the same keylogger, the output was sometimes different, so we needed to homogenize the format with the following scripts."
      ],
      "metadata": {
        "id": "imrUa27VH8Rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Process output files in incompatible formats"
      ],
      "metadata": {
        "id": "dEI9hgK_hOgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCTION TO CONVERT XML OUTPUT TO CSV\n",
        "import pandas as pd #to output csv\n",
        "import xml.etree.ElementTree as ET #to process xml files\n",
        "\n",
        "#Function to extract the data for each csv row in the xml file\n",
        "def extract_data_from_cells(row_element): #The row_element argument is supplied by the next function\n",
        "    cell_elements = row_element.findall('.//s:Cell', namespaces={\"s\": \"urn:schemas-microsoft-com:office:spreadsheet\"})\n",
        "    row_data = [cell.find('.//s:Data', namespaces={\"s\": \"urn:schemas-microsoft-com:office:spreadsheet\"}).text for cell in cell_elements]\n",
        "    return row_data\n",
        "\n",
        "#----------\n",
        "# MAIN\n",
        "#----------\n",
        "#Function to convert\n",
        "def convert_xml_to_csv(xml_file, csv_file): #Arguments: xml file to parse, desired output filename\n",
        "    # Parsing the xml\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    rows_data = [] #List to add the data for the csv\n",
        "\n",
        "    # Going through xml tree and finding where the csv row data is\n",
        "    for worksheet in root.findall('.//s:Worksheet', namespaces={\"s\": \"urn:schemas-microsoft-com:office:spreadsheet\"}):\n",
        "        for table in worksheet.findall('.//s:Table', namespaces={\"s\": \"urn:schemas-microsoft-com:office:spreadsheet\"}):\n",
        "            for row in table.findall('.//s:Row', namespaces={\"s\": \"urn:schemas-microsoft-com:office:spreadsheet\"}):\n",
        "                # Extracting the row data\n",
        "                row_data = extract_data_from_cells(row)\n",
        "                rows_data.append(row_data)\n",
        "\n",
        "    # Adding the data to a dataframe to output as csv\n",
        "    columns=['Time', 'Action', 'Key', 'Modifier', 'MouseClick', 'x', 'y', 'Task']\n",
        "    df = pd.DataFrame(rows_data, columns=columns)\n",
        "    # Savig the DataFrame to a CSV file\n",
        "    df.to_csv(csv_file, index=False)"
      ],
      "metadata": {
        "id": "HUAJefKPIhOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCTION TO CONVERT TXT OUTPUT TO CSV\n",
        "import pandas as pd #to output csv\n",
        "from datetime import datetime #to convert timestamps\n",
        "\n",
        "#Function to change the format of the actions (change \"moved\" to \"move\")\n",
        "def transform_actions_to_excel_format(action):\n",
        "    if action == 'Moved':\n",
        "        return 'Move'\n",
        "    elif action == 'Pressed':\n",
        "        return 'Pressed'\n",
        "    elif action == 'Key':\n",
        "        return 'Key'\n",
        "\n",
        "#Function to change timestamp format from dot to colon\n",
        "def transform_time_to_colon_format(timestamp):\n",
        "    input_format = '%Y-%m-%dT%H:%M:%S.%f'\n",
        "    desired_format = '%Y-%m-%dT%H:%M:%S:%f'\n",
        "    #Parsing the timestamp\n",
        "    datetime_obj = datetime.strptime(timestamp, input_format)\n",
        "    #Modifying the timestamp\n",
        "    formatted_timestamp = datetime_obj.strftime(desired_format)[:-3]\n",
        "    return formatted_timestamp\n",
        "\n",
        "#----------\n",
        "# MAIN\n",
        "#----------\n",
        "#Function to convert txt keylogger output to csv\n",
        "def convert_txt_to_csv_keylogger(txt_file,desired_csv_file): #Arguments: keylogger txt output file, filename for desired output csv file\n",
        "  #Open file as a dataframe\n",
        "  df = pd.read_csv(txt_file, sep=\"\\t\") #File elements are separated by tabs; revised on Notepad++\n",
        "\n",
        "  #Change Action format\n",
        "  df['Action'] = df['Action'].apply(transform_actions_to_excel_format)\n",
        "\n",
        "  #Create list of columns for new df: Time Action Key Modifier MouseClick X Y Task\n",
        "  new_df_time_list = []\n",
        "  new_df_action_list = []\n",
        "  new_df_key_list = []\n",
        "  new_df_modifier_list = []\n",
        "  new_df_mouseclick_list = []\n",
        "  new_df_x_list =[]\n",
        "  new_df_y_list = []\n",
        "  new_df_task_list = []\n",
        "\n",
        "  #Iterate through old df and add concent to lists for new df\n",
        "  #Action --> Key: move x value to Key column, y to modifier, others blank\n",
        "  #Action --> Pressed: move x value to MouseClick column, others blank\n",
        "  #Action --> Move: move x value to X column, y to Y, others blank\n",
        "  for index, row in df.iterrows():\n",
        "    if row['Action'] == 'Key':\n",
        "      new_df_time_list.append(row['Time'])\n",
        "      new_df_action_list.append(row['Action'])\n",
        "      new_df_key_list.append(row['X'])\n",
        "      new_df_modifier_list.append(row['Y'])\n",
        "      new_df_mouseclick_list.append('')\n",
        "      new_df_x_list.append('')\n",
        "      new_df_y_list.append('')\n",
        "      new_df_task_list.append('')\n",
        "    elif row['Action'] == 'Pressed':\n",
        "      new_df_time_list.append(row['Time'])\n",
        "      new_df_action_list.append(row['Action'])\n",
        "      new_df_key_list.append('')\n",
        "      new_df_modifier_list.append('')\n",
        "      new_df_mouseclick_list.append(row['X'])\n",
        "      new_df_x_list.append('')\n",
        "      new_df_y_list.append('')\n",
        "      new_df_task_list.append('')\n",
        "    elif row['Action'] == 'Move':\n",
        "      new_df_time_list.append(row['Time'])\n",
        "      new_df_action_list.append(row['Action'])\n",
        "      new_df_key_list.append('')\n",
        "      new_df_modifier_list.append('')\n",
        "      new_df_mouseclick_list.append('')\n",
        "      new_df_x_list.append(row['X'])\n",
        "      new_df_y_list.append(row['Y'])\n",
        "      new_df_task_list.append('')\n",
        "\n",
        "  #Create new df from the content added to the lists\n",
        "  new_df = pd.DataFrame(list(zip(new_df_time_list, new_df_action_list, new_df_key_list, new_df_modifier_list, new_df_mouseclick_list, new_df_x_list, new_df_y_list, new_df_task_list)), columns = ['Time', 'Action', 'Key', 'Modifier', 'MouseClick', 'X', 'Y', 'Task'])\n",
        "\n",
        "  #Change format of Time column\n",
        "  new_df['Time'] = new_df['Time'].apply(transform_time_to_colon_format)\n",
        "\n",
        "  #Export the dataframe to a csv file\n",
        "  new_df.to_csv(desired_csv_file, index=False)\n"
      ],
      "metadata": {
        "id": "wX_67RlHKfib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Process files already in homogenous format\n",
        "Even after converting all output files to the same format, we needed to transform the timestamps: keyloggers record timestamps in absolute time (clock time), whereas the transcripts timestamps are relative, reflecting the duration of the dialogue (e.g., 18:30 [6:30 pm] vs 02:34 [minute 2:34 of the recording]). We also then needed to reduce the granularity of the output: when the users moved the mouse, there could be tens of entries per second, one per each small shift of coordenates, but we don't need so much detail."
      ],
      "metadata": {
        "id": "pmW1EkB-hXXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCTION TO CHANGE TIMESTAMP FORMAT\n",
        "def change_timestamp_to_colon(keylog_csv,output_csv): #Arguments: csv file of keylogger output, desired output csv filename\n",
        "  import pandas as pd #to use csv files\n",
        "  from datetime import datetime #to process timestamps\n",
        "\n",
        "  # Read the keylog file\n",
        "  df = pd.read_csv(keylog_csv)\n",
        "\n",
        "  # Retrieve the absolute timestamp values from the \"Time\" column\n",
        "  absolute_timestamps_column_list = df['Time'].tolist()\n",
        "  formatted_timestamps = []\n",
        "  for absolute_timestamp in absolute_timestamps_column_list:\n",
        "    # Parse the absolute timestamp string to datetime object\n",
        "    initial_format_string = '%Y-%m-%dT%H:%M:%S.%f'\n",
        "    parsed_datetime = datetime.strptime(absolute_timestamp, initial_format_string)\n",
        "\n",
        "    # Desired format\n",
        "    final_format_string = '%Y-%m-%dT%H:%M:%S:%f'\n",
        "    # Convert timestamp to desired format and add to list\n",
        "    converted_datetime_string = parsed_datetime.strftime(final_format_string)\n",
        "    formatted_timestamps.append(converted_datetime_string)\n",
        "\n",
        "  # Add the formatted timestamps as a new column in the DataFrame\n",
        "  df['Formatted Timestamp'] = formatted_timestamps\n",
        "\n",
        "  # Write the updated DataFrame to a new file\n",
        "  f.to_csv(output_csv, index=False)\n"
      ],
      "metadata": {
        "id": "5te4KCwBkktY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCTION TO CONVERT KEYLOG TIMESTAMPS TO RELATIVE\n",
        "def convert_keylog_to_relative(keylog_csv, output_xls, starting_point): #Arguments: csv with keylog entries, desired output xls file, absolute time where dialogue starts\n",
        "#Starting point should be in format '%H:%M:%S:%f', e.g., '13:37:40:000'\n",
        "  import pandas as pd #to use csv files\n",
        "  from datetime import datetime #to process timestamps\n",
        "\n",
        "  # Read the keylog file\n",
        "  df = pd.read_csv(keylog_csv)\n",
        "\n",
        "  # Retrieve the absolute timestamp values from the \"Time\" column\n",
        "  absolute_timestamps_column_list = df['Formatted Timestamp'].tolist()\n",
        "\n",
        "  # Convert starting point timestamp to datetime object\n",
        "  starting_point_dt = datetime.strptime(starting_point, '%H:%M:%S:%f')\n",
        "\n",
        "  # Convert absolute timestamps to relative timestamps\n",
        "  relative_timestamps = []\n",
        "  for absolute_timestamp in absolute_timestamps_column_list:\n",
        "    # Parse the absolute timestamp string as datetime object\n",
        "    timestamp = datetime.strptime(str(absolute_timestamp), '%Y-%m-%dT%H:%M:%S:%f')\n",
        "\n",
        "    # Calculate the difference\n",
        "    difference = timestamp - starting_point_dt\n",
        "\n",
        "    # Extract hours, minutes, seconds, and milliseconds from difference\n",
        "    hours = difference.seconds // 3600\n",
        "    minutes = (difference.seconds // 60) % 60\n",
        "    seconds = difference.seconds % 60\n",
        "    milliseconds = difference.microseconds // 1000\n",
        "\n",
        "    # Format the relative timestamp string\n",
        "    relative_timestamp = '{:02d}:{:02d}:{:02d}:{:03d}'.format(hours, minutes, seconds, milliseconds)\n",
        "\n",
        "    relative_timestamps.append(relative_timestamp)\n",
        "\n",
        "  # Add the relative timestamps as a new column in the DataFrame\n",
        "  df['Relative Timestamp'] = relative_timestamps\n",
        "\n",
        "  # Write the updated DataFrame to a new Excel file\n",
        "  df.to_excel(output_xls, index=False, engine='xlsxwriter')"
      ],
      "metadata": {
        "id": "cvImvUhOirOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCTION TO REDUCE KEYLOG GRANULARITY\n",
        "#Keep all key and pressed data points But reduce the number of move actions\n",
        "\n",
        "def reduce_keylog_granularity(keylog_file,xls_or_csv,desired_output_csv): #Arguments: file of keylogger output with transformations, whether the file is xsls or csv,desired output csv filename\n",
        "  import pandas as pd #to use csv\n",
        "  import re #to find segments in strings and perform substitutions\n",
        "  from datetime import datetime, timedelta #to process timestamps\n",
        "  import numpy as np #to sort data\n",
        "\n",
        "  # Import data\n",
        "  if xls_or_csv == 'xls':\n",
        "    df = pd.read_excel(keylog_file)\n",
        "  if  xls_or_csv == 'csv':\n",
        "    df = pd.read_csv(keylog_file)\n",
        "\n",
        "  # Create lists from data\n",
        "  time_list = df['Time'].tolist()\n",
        "  key_list = df['Key'].tolist()\n",
        "  modifier_list = df['Modifier'].tolist()\n",
        "  mouseclick_list = df['MouseClick'].tolist()\n",
        "  x_list = df['X'].tolist()\n",
        "  y_list = df['Y'].tolist()\n",
        "  task_list = df['Task'].tolist()\n",
        "  relative_timestamp_list = df['Relative Timestamp'].tolist()\n",
        "\n",
        "  # Create list of indexes that we'll keep\n",
        "  keep_index_list = []\n",
        "  # Create dictionary of Move actions and their indexes\n",
        "  move_dict = {}\n",
        "\n",
        "  # Iterate through Action column\n",
        "  # If the action is Move, add the index as key to dictionary, the Time as value\n",
        "  # If the action is not Move, add the index to keep_index_list\n",
        "  for index, value in df['Action'].items():\n",
        "    if value != 'Move':\n",
        "      keep_index_list.append(index)\n",
        "    else:\n",
        "      move_dict[index] = df['Time'][index]\n",
        "\n",
        "  # Iterate through move_dict and convert timestamp strings to datetime objects\n",
        "  timestamp_objects = {}\n",
        "  for key, value in move_dict.items():\n",
        "    # (I use re.sub to replace the colon before the deciseconds with a dot, to match the standard format)\n",
        "    timestamp_objects[key] = datetime.strptime(re.sub(r\":(\\d{3})\", r\".\\1\", value), \"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "\n",
        "  # Adjust granularity\n",
        "  adjusted_timestamps = {} #Here I'll store the Move actions that I'll keep, before performing some transfromation\n",
        "  previous_timestamp = None\n",
        "  # I count one quarter of a second; I won't keep more than one Move action per quarter second\n",
        "  quarter_second = timedelta(milliseconds=250)\n",
        "  for key, timestamp in timestamp_objects.items():\n",
        "    # If it's the first timestamp, or it's more than 0.25 seconds apart from the previous one, I add it to the ones to keep\n",
        "    if previous_timestamp is None or (timestamp - previous_timestamp) >= quarter_second:\n",
        "      adjusted_timestamps[key] = timestamp\n",
        "      # And update the previous timestampt value to the latest iteration\n",
        "      previous_timestamp = timestamp\n",
        "\n",
        "  # Create dictionary to store transformed Move actions\n",
        "  keep_move_dict = {}\n",
        "\n",
        "  # Go through Move timestamps that I'll keep and convert datetime objects back to timestamp strings and store them in keep_move_dict\n",
        "  for key, timestamp in adjusted_timestamps.items():\n",
        "    keep_move_dict[key] = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S:%f\")[:-3]  # Remove last three digits\n",
        "\n",
        "  # Add the indexes of the keep_move_dict to the keep_index_list (get all the indexes that I'll keep in one single dictionary)\n",
        "  for key, value in keep_move_dict.items():\n",
        "    keep_index_list.append(key)\n",
        "\n",
        "  # Create new dataframe with only the rows we want to keep from the old df\n",
        "  df_new = df.iloc[keep_index_list]\n",
        "\n",
        "  # Sort the new dataframe by Time. First I find the desired index order\n",
        "  sort_indices = np.argsort(df_new['Time'])\n",
        "  # Use the indices to rearrange the rows of the entire new df and create sorted df\n",
        "  df_sorted = df_new.iloc[sort_indices]\n",
        "\n",
        "  # Export to csv\n",
        "  df_sorted.to_csv(desired_output_csv, index=False)"
      ],
      "metadata": {
        "id": "4XYvFqDBnF1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Adding keylogger entries to json file"
      ],
      "metadata": {
        "id": "xxQu3oL1zRtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCTION TO ADD SPEAKER LABELS AND MERGE CSVs AND ORDER CHRONOLOGICALLY\n",
        "def merge_and_order_csvs(csv_a, csv_b, speaker_a_number, speaker_b_number): #Arguments: keylogger csv for speaker A, keylogger csv for speaker B, code for speaker A, code for speaker B\n",
        "#Example arguments: '012_Keylog_Trimmed_synchronized_reduced.csv', '011_Keylog_Trimmed_synchronized_reduced.csv','012','011'\n",
        "#Arguments are fed by next function\n",
        "  import json #to add to json file\n",
        "  from datetime import datetime #to process timestamps\n",
        "  import numpy as np #to sort data\n",
        "  import pandas as pd #to use csv\n",
        "\n",
        "  # Read the CSV files\n",
        "  df_A = pd.read_csv(csv_a)\n",
        "  df_B = pd.read_csv(csv_b)\n",
        "\n",
        "  # Add speaker label to each keylogger entry before we merge them and forget who typed what\n",
        "  df_A['Speaker'] = speaker_a_number\n",
        "  df_B['Speaker'] = speaker_b_number\n",
        "\n",
        "  # Merge the CSV files\n",
        "  df_merged = pd.concat([df_A, df_B])\n",
        "\n",
        "  # Iterate through df_merged and convert the timestamp column to datetime format\n",
        "  datetime_list = []\n",
        "  for index, row in df_merged.iterrows():\n",
        "    time_string = row['Time'].strip() #Remove leading/trailing whitespace\n",
        "    time_dt = datetime.strptime(time_string, '%Y-%m-%dT%H:%M:%S:%f')\n",
        "    datetime_list.append(time_dt)\n",
        "  # Add datetime objects to new column\n",
        "  df_merged['Time_dt'] = datetime_list\n",
        "\n",
        "  #Sort the DataFrame based on the Time_dt column. First get the indices that would sort the 'Time_dt' column\n",
        "  sort_indices = np.argsort(df_merged['Time_dt'])\n",
        "  #Use the indices to rearrange the rows of the entire DataFrame\n",
        "  df_sorted = df_merged.iloc[sort_indices]\n",
        "  return df_sorted"
      ],
      "metadata": {
        "id": "nhQm-_VeznVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------------\n",
        "# MAIN\n",
        "#------------\n",
        "#FUNCTION TO GO THROUGH JSON AND ADD SILENT TURNS AND KEYLOGS\n",
        "# Some keylogger data is added to turns without transcripts (silent turns created for when users are active on the computer, but not speaking)\n",
        "def add_keylogs_to_json(csv_a, csv_b, speaker_a_number, speaker_b_number, json_file_path):\n",
        "#Arguments: keylogger csv for speaker A, keylogger csv for speaker B, code for speaker A, code for speaker B, filepath of json file with transcripts from dialogue between A and B\n",
        "  #Read the CSV files, merge them and sort theentries chronologically\n",
        "  df_merged_sorted = merge_and_order_csvs(csv_a, csv_b, speaker_a_number, speaker_b_number)\n",
        "\n",
        "  # Load the json file\n",
        "  with open(json_file_path, 'r') as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "\n",
        "  # Extract the session ID from initial json\n",
        "  session_id = json_data['SESSION']['sessionID']\n",
        "\n",
        "  # Sort the TURN objects by their start timestamps\n",
        "  sorted_json_data = sorted(json_data['SESSION']['TURNS'], key=lambda x: x['TURN']['TIME']['START'])\n",
        "\n",
        "  list_to_add_to_json_as_silent_turns = []\n",
        "\n",
        "  # Iterate through TURN objects with access to previous and next. This converts the json data to list of dictionaries\n",
        "  #The index works basically as a counter, so it starts at 0 and goes up by 1 for each turn. But it can also be used to access the previous and next turn\n",
        "  #The problem is that we can't add things to the json directly, so we need to add them to a list\n",
        "  #And then add the list to the json at the end, and then we need to sort the json by start time\n",
        "  for idx, turn_obj in enumerate(sorted_json_data):\n",
        "    keylogs_for_turn = [] #Here I'll store the keylog entries that correspond to this turn\n",
        "    keylogs_for_silent_turn = [] #Here I'll store the keylog entries that correspond to a new turn without dialogue\n",
        "\n",
        "    # Extract turn start and end times\n",
        "    start_time_str = turn_obj['TURN']['TIME']['START']\n",
        "    end_time_str = turn_obj['TURN']['TIME']['END']\n",
        "    # Convert timestamp strings to datetime objects\n",
        "    start_time = datetime.strptime(start_time_str, '%H:%M:%S,%f')\n",
        "    end_time = datetime.strptime(end_time_str, '%H:%M:%S,%f')\n",
        "\n",
        "    # Access the previous TURN (if it exists)\n",
        "    if idx > 0:\n",
        "      previous_turn = sorted_json_data[idx - 1]\n",
        "    else:\n",
        "      previous_turn = None\n",
        "\n",
        "    # If there is a previous turn, see when it ended (to know period between end of previous turn and start of current)\n",
        "    if previous_turn != None:\n",
        "      prev_end_time_str = previous_turn['TURN']['TIME']['END']\n",
        "      prev_end_time = datetime.strptime(prev_end_time_str, '%H:%M:%S,%f')\n",
        "\n",
        "    #Check if this we're in the first or last turn\n",
        "    #FIRST TURN\n",
        "    if idx == 0:\n",
        "      for index, row in df_merged_sorted.iterrows():\n",
        "        csv_timestamp = row['Relative Timestamp']\n",
        "        # Convert tiemstampt to datetime object\n",
        "        csv_timestamp_clock_dt = datetime.strptime(csv_timestamp, \"%H:%M:%S:%f\")\n",
        "        #If the keylog timestamp is within the turn timeframe, add it to the turn\n",
        "        if start_time <= csv_timestamp_clock_dt <= end_time:\n",
        "          keylog_entry = str(row['Relative Timestamp']) + ' ' + str(row['Time']) + ' '  + str(row['Speaker']) + ' ' + row['Action'] + ' ' + str(row['Key']) + ' ' + str(row['Modifier']) + ' ' + str(row['MouseClick']) + ' ' + str(row['X']) + ' ' + str(row['Y'])\n",
        "          keylogs_for_turn.append(keylog_entry)\n",
        "        # If the keylog timestamp goes before the first turn, add it as silent turn\n",
        "        if csv_timestamp_clock_dt < start_time:\n",
        "          keylog_entry = str(row['Time']) + ' '  + str(row['Speaker']) + ' ' + row['Action'] + ' ' + str(row['Key']) + ' ' + str(row['Modifier']) + ' ' + str(row['MouseClick']) + ' ' + str(row['X']) + ' ' + str(row['Y'])\n",
        "          keylogs_for_silent_turn.append(keylog_entry)\n",
        "        # If there are any keylog entires to add, add silent turn\n",
        "        silent_turn = {\n",
        "            'TURN':{\n",
        "              'TIME': {\n",
        "                'START': '00:00:00,000',\n",
        "                'END': start_time_str\n",
        "                                    },\n",
        "              'KEYLOGS': keylogs_for_silent_turn\n",
        "            }\n",
        "          }\n",
        "        if len(keylogs_for_silent_turn) > 0:\n",
        "          list_to_add_to_json_as_silent_turns.append(silent_turn)\n",
        "      #Add keylog to normal turn\n",
        "      if len(keylogs_for_turn) > 0:\n",
        "        turn_obj['TURN']['KEYLOGS'] = keylogs_for_turn\n",
        "\n",
        "    #LAST TURN\n",
        "    elif idx == len(json_data) - 1:\n",
        "      list_of_relative_timestamps = [] #I'll use this to then find the last timestamp to establish as turn end if it's a silent turn\n",
        "      for index, row in df_merged_sorted.iterrows():\n",
        "        csv_timestamp = row['Relative Timestamp']\n",
        "        list_of_relative_timestamps.append(csv_timestamp)\n",
        "        csv_timestamp_clock_dt = datetime.strptime(csv_timestamp, \"%H:%M:%S:%f\")\n",
        "        #If the keylog timestamp is within the turn timeframe, add it to the turn\n",
        "        if start_time <= csv_timestamp_clock_dt <= end_time:\n",
        "          keylog_entry = str(row['Relative Timestamp']) + ' ' + str(row['Time']) + ' '  + str(row['Speaker']) + ' ' + row['Action'] + ' ' + str(row['Key']) + ' ' + str(row['Modifier']) + ' ' + str(row['MouseClick']) + ' ' + str(row['X']) + ' ' + str(row['Y'])\n",
        "          keylogs_for_turn.append(keylog_entry)\n",
        "        #If the keylog timestamp goes after the last turn, add it as silent turn\n",
        "        if csv_timestamp_clock_dt > end_time:\n",
        "          keylog_entry = str(row['Relative Timestamp']) + ' ' + str(row['Time']) + ' '  + str(row['Speaker']) + ' ' + row['Action'] + ' ' + str(row['Key']) + ' ' + str(row['Modifier']) + ' ' + str(row['MouseClick']) + ' ' + str(row['X']) + ' ' + str(row['Y'])\n",
        "          keylogs_for_silent_turn.append(keylog_entry)\n",
        "\n",
        "      # If there are any keylog entires to add, add silent turn\n",
        "      if len(keylogs_for_silent_turn) > 0:\n",
        "        silent_turn = {\n",
        "            'TURN':{\n",
        "              'TIME': {\n",
        "              'START': end_time_str,\n",
        "              'END': list_of_relative_timestamps[-1]\n",
        "              },\n",
        "              'KEYLOGS': keylogs_for_silent_turn\n",
        "              }\n",
        "          }\n",
        "        list_to_add_to_json_as_silent_turns.append(silent_turn)\n",
        "      #Add keylog to normal turn\n",
        "      if len(keylogs_for_turn) > 0:\n",
        "        turn_obj['TURN']['KEYLOGS'] = keylogs_for_turn\n",
        "\n",
        "    #NORMAL TURN\n",
        "    else:\n",
        "      # Go thru csv to find keylogs inside this turn timeframe\n",
        "      for index, row in df_merged_sorted.iterrows():\n",
        "        csv_timestamp = row['Relative Timestamp']\n",
        "        csv_timestamp_clock_dt = datetime.strptime(csv_timestamp, \"%H:%M:%S:%f\")\n",
        "        # If the keylog timestamp is within the turn timeframe, add it to the turn\n",
        "        if start_time <= csv_timestamp_clock_dt <= end_time:\n",
        "          keylog_entry = str(row['Relative Timestamp']) + ' ' + str(row['Time']) + ' '  + str(row['Speaker']) + ' ' + row['Action'] + ' ' + str(row['Key']) + ' ' + str(row['Modifier']) + ' ' + str(row['MouseClick']) + ' ' + str(row['X']) + ' ' + str(row['Y'])\n",
        "          keylogs_for_turn.append(keylog_entry)\n",
        "        # If the keylog timestamp is before this turn, but after the previous one, add as silent turn\n",
        "        if prev_end_time < csv_timestamp_clock_dt < start_time:\n",
        "          keylog_entry = str(row['Relative Timestamp']) + ' ' + str(row['Time']) + ' '  + str(row['Speaker']) + ' ' + row['Action'] + ' ' + str(row['Key']) + ' ' + str(row['Modifier']) + ' ' + str(row['MouseClick']) + ' ' + str(row['X']) + ' ' + str(row['Y'])\n",
        "          keylogs_for_silent_turn.append(keylog_entry)\n",
        "\n",
        "      # Add silent turn to list\n",
        "      if len(keylogs_for_silent_turn) > 0:\n",
        "        silent_turn = {\n",
        "                  'TURN':{\n",
        "                        'TIME': {\n",
        "                          'START': prev_end_time_str,\n",
        "                          'END': start_time_str\n",
        "                                    },\n",
        "                      'KEYLOGS': keylogs_for_silent_turn\n",
        "                      }\n",
        "                  }\n",
        "        list_to_add_to_json_as_silent_turns.append(silent_turn)\n",
        "      # Add keylog to normal turn\n",
        "      if len(keylogs_for_turn) > 0:\n",
        "        turn_obj['TURN']['KEYLOGS'] = keylogs_for_turn\n",
        "\n",
        "  # Add silent turns to json (sorted_json_data, as that's where I added the keylog for normal turns)\n",
        "  for silent_turn in list_to_add_to_json_as_silent_turns:\n",
        "    sorted_json_data.append(silent_turn)\n",
        "  # Save turns json_data to a separate file (we'll then merge files)\n",
        "  filename_to_save_turns = json_file_path[:-5] + ' - keylog turns.json'\n",
        "  with open(filename_to_save_turns, 'w') as json_file:\n",
        "    json.dump(sorted_json_data, json_file, indent=4)\n",
        "\n",
        "  # Reopen turns json file and sort by start time\n",
        "  with open(filename_to_save_turns, 'r') as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "\n",
        "  # Remove duplicates from list of turns\n",
        "  #First sort json\n",
        "  sorted_json_data = sorted(json_data, key=lambda x: x['TURN']['TIME']['START'])\n",
        "  #Then create list of unique turns and append only turns not seen before in loop\n",
        "  sorted_json_data_unique_list = list()\n",
        "  for sorted_turn in sorted_json_data:\n",
        "    if sorted_turn not in sorted_json_data_unique_list:\n",
        "      sorted_json_data_unique_list.append(sorted_turn)\n",
        "\n",
        "  #Save json_data to file. Create new json with all the data\n",
        "  annotation_json = {\n",
        "        \"SESSION\": {\n",
        "            \"sessionID\": session_id,\n",
        "            \"TURNS\": sorted_json_data_unique_list\n",
        "        }\n",
        "  }\n",
        "  filename_to_save_json_sorted = json_file_path[:-5] + ' - with keylog and sorted.json'\n",
        "  with open(filename_to_save_json_sorted, 'w') as json_file:\n",
        "    json.dump(annotation_json, json_file, indent=4)\n",
        "  print('Done with', filename_to_save_json_sorted)"
      ],
      "metadata": {
        "id": "nYSd_bCfzyFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adding the code data\n",
        "We record the code that the participants create using the [LocalHistory plugin](https://marketplace.visualstudio.com/items?itemName=xyz.local-history) of VSCode. This plugin records a new file every 1-2 seconds if there has been any change in the code (it can be as little as 1 second, but since the code files do not record the time down to the miliseconds, the time difference may be 2 seconds between timestamps rounded to the second). The files are recorded with a timestamp in their filename.<br/>\n",
        "When adding the code data to our json files, we want to add:\n",
        "- The code as a string (for direct processing)\n",
        "- The code filename (so that it may be retrieved)\n",
        "- The timestamp (for quality control, as well as to know precisely at which point in a turn changes were made; we want not jsut the absolute timestamp from the filename, but a relative timestamp that is comparable to the transcript timestamps)\n",
        "- The syntax trees of the code file (this structural information has proven useful in NLP tasks (e.g., [Oda et al, 2015](http://dx.doi.org/10.1109/ASE.2015.36)))."
      ],
      "metadata": {
        "id": "_qm7ytVWJeX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When extracting code trees, we do not extract them directly, but rather segment the code first (we segment it into main indented sections and independent statement outside indented segments; we do not split nested indented segments). We use segmentation for two reasons:\n",
        "<ul>\n",
        "<li>We are dealing with code in the process of being written. It is thus incomplete, so it won't generate output, as the parser can only handle syntactically correct code. Therefore, for any segment that we try to parse, we do it as a try statement.</li>\n",
        "<li>We extracted the trees of some sample code and found it not very informative (e.g., some programs yielded the same kind of output as a comment).</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "gxYzKWFNOa5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCTION TO EXTRACT CODE TREES\n",
        "def turn_code_into_tree(folder): #Argument: folder where code files are stored\n",
        "  import os #to browse through folder\n",
        "  import ast #to generate syntax trees\n",
        "  import re #to find string segments in code\n",
        "\n",
        "  # Find code files in folder\n",
        "  python_files_in_folder = [f for f in os.listdir(folder) if f.endswith('.py')]\n",
        "  for python_file in python_files_in_folder:\n",
        "    # Define tree filename based on code filename\n",
        "    tree_filename = python_file[:-3] + \"_tree.txt\"\n",
        "    # Obtain filepaths\n",
        "    full_file_path = os.path.join(folder, python_file)\n",
        "    tree_file_path = os.path.join(folder, tree_filename)\n",
        "    # Open code file\n",
        "    with open(full_file_path, 'r') as f:\n",
        "      code_list = [] #List to store each line of code\n",
        "      indented_list = [] #List to store indented bits before appending to main list\n",
        "      code_readlines = f.readlines() #Extract list of code lines\n",
        "      #Filling up list of code lines\n",
        "      for line in iter(code_readlines): #I use iter to be able to look at next line\n",
        "        #If the current line belongs to and indented bit, put it in special list\n",
        "        if bool(re.search(\":$\", line)) == True or bool(re.search(\"^   \", line)) == True:\n",
        "          indented_list.append(line)\n",
        "        #If this is the end of the indented bit, append special list to main list as one string\n",
        "        if bool(re.search(\":$\", next(iter(code_readlines)))) == False and bool(re.search(\"^   \", next(iter(code_readlines)))) == False:\n",
        "          code_list.append(str(indented_list))\n",
        "        else: #If this is not an indented bit, but an independent code statement\n",
        "          #Empty the list of indented bits - we're outide a function/etc. now\n",
        "          indented_list = []\n",
        "          code_list.append(line)\n",
        "\n",
        "\n",
        "      #Returning tree in string format\n",
        "      whole_tree_string = \"\"\n",
        "      # Iterate through list of code segments to extract trees\n",
        "      for codeline in code_list:\n",
        "        codeline_string = str(codeline)\n",
        "        try: #Try because the parser raises errors for any code that has bad syntax/is incomplete\n",
        "          # Parse line\n",
        "          tree_string = ast.parse(codeline_string)\n",
        "          # If a previous segment was already parsed, don't overwrite tree, but add new tree as new line\n",
        "          if len(whole_tree_string) > 0:\n",
        "            whole_tree_string = whole_tree_string + \"\\n\" + ast.dump(tree_string)\n",
        "          else:\n",
        "            whole_tree_string = ast.dump(tree_string)\n",
        "        except: # If segment can't be parsed, skip\n",
        "          pass\n",
        "  print(whole_tree_string, file=open(tree_file_path, \"w\"))\n",
        "  return print(\"done\")\n"
      ],
      "metadata": {
        "id": "UQr0FFteLOJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUNCTION TO MAKE TIMESTAMP RELATIVE\n",
        "def make_timestamp_relative(timestamp, starting_point): #Arguments: timestamp to convert, dialogue starting point. Arguments provided by main function\n",
        "#Starting_point is a string in the format '13:37:40,000'\n",
        "  from datetime import datetime, timedelta #to process timestamps\n",
        "  # Starting point timestamp\n",
        "  starting_point = datetime.strptime(starting_point, '%H:%M:%S,%f')\n",
        "\n",
        "  # Calculate the difference\n",
        "  difference = timestamp - starting_point\n",
        "\n",
        "  # Extract hours, minutes, seconds, and milliseconds of the difference\n",
        "  hours = difference.seconds // 3600\n",
        "  minutes = (difference.seconds // 60) % 60\n",
        "  seconds = difference.seconds % 60\n",
        "  milliseconds = difference.microseconds // 1000\n",
        "\n",
        "  # Format the relative timestamp string\n",
        "  relative_timestamp = '{:02d}:{:02d}:{:02d},{:03d}'.format(hours, minutes, seconds, milliseconds)\n",
        "  relative_timestamp_datetime = datetime.strptime(relative_timestamp, '%H:%M:%S,%f')\n",
        "\n",
        "  return relative_timestamp_datetime\n",
        "\n",
        "#-----------------\n",
        "# MAIN\n",
        "#-----------------\n",
        "#FUNCTION TO ADD CODE AND TREES TO JSON\n",
        "def add_code_to_json(code_folder, json_file, starting_point): #Arguments: folder where code is found, json file to append code entries, timestamp where dialogue starts\n",
        "#Starting_point is a string in the format '13:37:40,000'\n",
        "  import os #to browse folder\n",
        "  import json #to process json file\n",
        "  from datetime import datetime, timedelta #to process timestamps\n",
        "\n",
        "  # Load JSON data\n",
        "  with open(json_file, 'r') as json_file:\n",
        "    session_data = json.load(json_file)\n",
        "\n",
        "  # Collect code filepaths\n",
        "  code_data_filename_list = []\n",
        "  for root, dirs, files in os.walk(code_folder):\n",
        "    for file in files:\n",
        "      if file.endswith('.py'): # Only code files\n",
        "        code_filename_full = os.path.join(root, file) # Need to join the path to the filename\n",
        "          code_data_filename_list.append(code_filename_full)\n",
        "\n",
        "  # Iterate through turns\n",
        "  for turn_obj in session_data['SESSION']['TURNS']:\n",
        "    # Extract turn timestamps\n",
        "    start_time = datetime.strptime(turn_obj['TURN']['TIME']['START'], '%H:%M:%S,%f')\n",
        "    end_time = datetime.strptime(turn_obj['TURN']['TIME']['END'], '%H:%M:%S,%f')\n",
        "\n",
        "    code = [] # here I'll add the code for this turn\n",
        "    # Iterate through all code files to find files for this turn\n",
        "    for code_filename_full in code_data_filename_list:\n",
        "      # Extract tree filepath from code filepath\n",
        "      tree_file = code_filename_full[:-3] + '_tree.txt' #[:-3] removes .py extension\n",
        "      with open(code_filename_full, 'r') as code_file:\n",
        "        # Get the filename\n",
        "        code_file_nametype = code_file.name # Extract the actual filename from the TextIOWrapper\n",
        "        code_filename_base = os.path.splitext(os.path.basename(code_file_nametype))[0] # Remove folder and file extension\n",
        "        # If it's an edited code filen, not the isntruction file\n",
        "        if len(code_filename_base) >2: #I want to exclude exercise files without timestamps (originals without edits)\n",
        "          # Extract timestamp from filename\n",
        "          code_timestamp = code_filename_base.split('_', 1)[1] #Remove exercise number and underscore\n",
        "          # Convert to datetime\n",
        "          code_time_datetime = datetime.strptime(code_timestamp, \"%Y%m%d%H%M%S\")\n",
        "          # Format\n",
        "          code_time_formatted = code_time_datetime.strftime('%H:%M:%S,%f')[:-3]  # Removing the last 3 digits of microseconds\n",
        "          # Convert back to datetime\n",
        "          code_time_formatted_datetime = datetime.strptime(code_time_formatted, '%H:%M:%S,%f')\n",
        "          # Convert to relative\n",
        "          code_time_relative = make_timestamp_relative(code_time_formatted_datetime, starting_point)\n",
        "          # Check if it's within turn time period\n",
        "          if start_time <= code_time_relative <= end_time:\n",
        "            # Load tree file too\n",
        "            with open(tree_file, 'r') as tree_file:\n",
        "              tree = tree_file.read()\n",
        "              # Design code entry\n",
        "              code_entry = {\n",
        "                'TIME': code_time_relative.strftime('%H:%M:%S,%f')[:-3],\n",
        "                'FILE': code_filename_base,\n",
        "                'CODE': code_file.read(),\n",
        "                'TREE': tree\n",
        "                }\n",
        "              # Add this entry to lsit of code for this turn\n",
        "              code.append(code_entry)\n",
        "    # After going through all the code, add the corresponding code entries to this turn\n",
        "    turn_obj['TURN']['CODE ENTRIES'] = code\n",
        "\n",
        "  # Outpput json file\n",
        "  with open(json_file.name, 'w') as json_output:\n",
        "    json.dump(session_data, json_output, indent=4)\n",
        "\n",
        "  print('Done with ' + json_file.name)\n"
      ],
      "metadata": {
        "id": "e7eLIztjU1cN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}